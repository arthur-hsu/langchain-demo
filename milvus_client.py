from langchain_milvus import Milvus
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain_openai import ChatOpenAI, OpenAI
import os
from langchain.chains import RetrievalQA

# 1. è¨­å®š Milvus ä¼ºæœå™¨
MILVUS_URI = "http://localhost:19530"
COLLECTION_NAME = "pdf_embeddings"

# 2. è¨­å®š Hugging Face Embeddings
model_name = "BAAI/bge-m3"
model_kwargs = {'device': 'mps'}
encode_kwargs = {'normalize_embeddings': True}

embedding_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# 3. é€£æ¥ Milvus å‘é‡è³‡æ–™åº«
vector_db = Milvus(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    connection_args={"uri": MILVUS_URI},
)

query_text = "LoRaWan Class B ä»‹ç´¹"
docs = vector_db.similarity_search(query_text, k=5)  # å–å‰ 5 ç­†æœ€ç›¸é—œçš„çµæœ

# å°å‡ºçµæœ
print("\nğŸ” æŸ¥è©¢çµæœï¼š")
for i, doc in enumerate(docs):
    print(f"ğŸ“„ {i+1}: {doc.page_content}\n")



# in_docker = True if os.getenv("IN_DOCKER", False) is not False else False
# url = "http://host.docker.internal:11434" if in_docker else "http://localhost:11434"
#
#
# # 4. ä½¿ç”¨ LangChain OpenAI èª¿ç”¨æœ¬åœ° Ollama
# llm = ChatOpenAI(model="deepseek-r1:14b", base_url=f"{url}/v1", api_key="ollama",streaming=True)
#
#
# # 5. å»ºç«‹ RAG æŸ¥è©¢æµç¨‹
# retriever = vector_db.as_retriever(search_kwargs={"k": 5})  # å–å› 5 å€‹æœ€ç›¸é—œçš„å…§å®¹
# rag_chain = RetrievalQA(llm=llm, retriever=retriever)
#
# # 6. æ¸¬è©¦ RAG å•ç­”
# query = "LoRaWan class b ä»‹ç´¹"
# result = rag_chain.run(query)
#
# print("RAG ç”Ÿæˆçš„ç­”æ¡ˆï¼š", result)
#
